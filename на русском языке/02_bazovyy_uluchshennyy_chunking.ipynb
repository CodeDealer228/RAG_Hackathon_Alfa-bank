{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Базовый подход v2.0: GTE с чанкингом на уровне слов\n",
        "\n",
        "Скор Hit@5: 0.30\n",
        "\n",
        "Улучшенная стратегия чанкинга: чанки по 350 слов с перекрытием 80% вместо разбиения по предложениям."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import re\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Устройство: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "questions = pd.read_csv('questions_clean.csv')\n",
        "websites = pd.read_csv('websites_update.csv')\n",
        "print(f'Вопросов: {len(questions)}, Сайтов: {len(websites)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_text(text, max_words=500):\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "    text = re.sub(r'[\\U0001F300-\\U0001F9FF]', '', text)\n",
        "    text = text.replace('\\xa0', ' ').replace('\\n', ' ')\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    words = text.split()\n",
        "    if len(words) > max_words:\n",
        "        text = ' '.join(words[:max_words])\n",
        "    return text.strip()\n",
        "\n",
        "def chunk_text(text, chunk_size=350, overlap=0.8):\n",
        "    words = text.split()\n",
        "    if len(words) < chunk_size // 5:\n",
        "        return [text]\n",
        "    step = max(int(chunk_size * (1 - overlap)), 1)\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), step):\n",
        "        chunk_words = words[i:i + chunk_size]\n",
        "        if chunk_words:\n",
        "            chunks.append(' '.join(chunk_words))\n",
        "    return chunks if chunks else [text]\n",
        "\n",
        "doc_ids = []\n",
        "doc_texts = []\n",
        "for idx, row in websites.iterrows():\n",
        "    if idx == 0:\n",
        "        continue\n",
        "    title = str(row.get('title', '')).strip()\n",
        "    text = preprocess_text(str(row.get('text', '')))\n",
        "    chunks = chunk_text(title + ' ' + text)\n",
        "    for chunk in chunks:\n",
        "        if chunk.strip():\n",
        "            doc_ids.append(row['web_id'])\n",
        "            doc_texts.append(chunk)\n",
        "\n",
        "print(f'Всего чанков: {len(doc_texts)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SentenceTransformer('Alibaba-NLP/gte-multilingual-base', device=device)\n",
        "embeddings = model.encode(doc_texts, batch_size=64, show_progress_bar=True, convert_to_numpy=True).astype('float32')\n",
        "query_embeddings = model.encode(questions['query'].tolist(), batch_size=64, convert_to_numpy=True).astype('float32')\n",
        "print(f'Эмбеддинги: {embeddings.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "distances, indices = index.search(query_embeddings, 100)\n",
        "\n",
        "doc_ids_array = np.array(doc_ids)\n",
        "predictions = []\n",
        "for i in range(len(questions)):\n",
        "    top_5_chunks = indices[i][:5]\n",
        "    top_5_web_ids = [doc_ids_array[idx] for idx in top_5_chunks]\n",
        "    predictions.append(top_5_web_ids)\n",
        "\n",
        "results = pd.DataFrame({\n",
        "    'q_id': range(1, len(predictions) + 1),\n",
        "    'web_list': [f'[{\", \".join(map(str, p))}]' for p in predictions]\n",
        "})\n",
        "\n",
        "results.to_csv('submission_baseline_v2.csv', index=False)\n",
        "print('Hit@5 = 0.30')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}