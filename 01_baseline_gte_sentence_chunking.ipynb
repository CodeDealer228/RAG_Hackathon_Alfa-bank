{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline Approach v1.0: GTE Embeddings with Sentence Chunking\n",
        "\n",
        "Hit@5 Score: 0.10\n",
        "\n",
        "This notebook implements the initial RAG pipeline using GTE embeddings with sentence-level chunking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import re\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "questions = pd.read_csv('questions_clean.csv')\n",
        "websites = pd.read_csv('websites_update.csv')\n",
        "print(f'Questions: {len(questions)}')\n",
        "print(f'Websites: {len(websites)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chunk_by_sentences(text, max_chars=1000):\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "    \n",
        "    for sent in sentences:\n",
        "        sent = sent.strip()\n",
        "        if not sent:\n",
        "            continue\n",
        "        if current_length + len(sent) > max_chars and current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = [sent]\n",
        "            current_length = len(sent)\n",
        "        else:\n",
        "            current_chunk.append(sent)\n",
        "            current_length += len(sent)\n",
        "    \n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "    return chunks\n",
        "\n",
        "doc_ids = []\n",
        "doc_texts = []\n",
        "for idx, row in websites.iterrows():\n",
        "    if idx == 0:\n",
        "        continue\n",
        "    chunks = chunk_by_sentences(str(row['text']))\n",
        "    for chunk in chunks:\n",
        "        if chunk.strip():\n",
        "            doc_ids.append(row['web_id'])\n",
        "            doc_texts.append(chunk)\n",
        "\n",
        "print(f'Total chunks: {len(doc_texts)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SentenceTransformer('Alibaba-NLP/gte-multilingual-base', device=device)\n",
        "embeddings = model.encode(doc_texts, batch_size=64, show_progress_bar=True, convert_to_numpy=True)\n",
        "embeddings = embeddings.astype('float32')\n",
        "print(f'Embeddings shape: {embeddings.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings)\n",
        "\n",
        "query_embeddings = model.encode(questions['query'].tolist(), batch_size=64, convert_to_numpy=True).astype('float32')\n",
        "distances, indices = index.search(query_embeddings, 100)\n",
        "\n",
        "print(f'Search completed. Shape: {indices.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc_ids_array = np.array(doc_ids)\n",
        "predictions = []\n",
        "for i in range(len(questions)):\n",
        "    top_5_chunks = indices[i][:5]\n",
        "    top_5_web_ids = [doc_ids_array[idx] for idx in top_5_chunks]\n",
        "    predictions.append(top_5_web_ids)\n",
        "\n",
        "results = pd.DataFrame({\n",
        "    'q_id': range(1, len(predictions) + 1),\n",
        "    'web_list': [str(p) for p in predictions]\n",
        "})\n",
        "\n",
        "results.to_csv('submission_baseline_v1.csv', index=False)\n",
        "print('Submission saved')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}