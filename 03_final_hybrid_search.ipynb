{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Approach: Hybrid Search with Query Expansion and Learning-to-Rank\n",
        "\n",
        "Hit@5 Score: 0.65\n",
        "\n",
        "Advanced methodology combining BM25 lexical search, Dense embeddings with Query Expansion, and CrossEncoder reranking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sentence_transformers import CrossEncoder\n",
        "from rank_bm25 import BM25Okapi\n",
        "import faiss\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "questions = pd.read_csv('questions_clean.csv')\n",
        "websites = pd.read_csv('websites_update.csv')\n",
        "print(f'Questions: {len(questions)}, Websites: {len(websites)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FINANCE_SYNONYMS = {\n",
        "    'вклад': ['вклад', 'депозит', 'накопление', 'сбережение'],\n",
        "    'кредит': ['кредит', 'заём', 'ссуда', 'микрокредит'],\n",
        "    'инвестиция': ['инвестиция', 'инвестирование', 'вложение', 'портфель'],\n",
        "    'карта': ['карта', 'пластиковая карта', 'кредитная карта', 'дебетовая'],\n",
        "    'счёт': ['счёт', 'счет', 'лицевой счёт', 'расчётный'],\n",
        "    'акция': ['акция', 'акции', 'ценная бумага', 'фондовый'],\n",
        "    'облигация': ['облигация', 'облигации', 'ценная бумага'],\n",
        "    'страховка': ['страховка', 'страхование', 'полис', 'страховой'],\n",
        "    'ипотека': ['ипотека', 'ипотечный', 'жилищный кредит'],\n",
        "}\n",
        "\n",
        "def expand_query(query):\n",
        "    expanded = str(query).lower()\n",
        "    for keyword, synonyms in FINANCE_SYNONYMS.items():\n",
        "        if keyword in expanded:\n",
        "            expanded += ' ' + ' '.join(synonyms)\n",
        "    return expanded\n",
        "\n",
        "questions['query_expanded'] = questions['query'].apply(expand_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_text(text, max_words=500):\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "    text = re.sub(r'[\\U0001F300-\\U0001F9FF]', '', text)\n",
        "    text = text.replace('\\xa0', ' ').replace('\\n', ' ')\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    words = text.split()\n",
        "    if len(words) > max_words:\n",
        "        text = ' '.join(words[:max_words])\n",
        "    return text.strip().lower()\n",
        "\n",
        "doc_ids = []\n",
        "doc_texts = []\n",
        "for idx, row in websites.iterrows():\n",
        "    if idx == 0:\n",
        "        continue\n",
        "    title = str(row.get('title', '')).strip()\n",
        "    text = preprocess_text(str(row.get('text', '')))\n",
        "    doc_text = title + ' ' + text + ' ' + title\n",
        "    doc_ids.append(row['web_id'])\n",
        "    doc_texts.append(doc_text)\n",
        "\n",
        "doc_ids_array = np.array(doc_ids)\n",
        "print(f'Documents prepared: {len(doc_texts)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_docs = [doc.split() for doc in doc_texts]\n",
        "bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "bm25_scores_all = []\n",
        "for query in tqdm(questions['query_expanded'], desc='BM25'):\n",
        "    tokens = str(query).lower().split()\n",
        "    scores = bm25.get_scores(tokens)\n",
        "    bm25_scores_all.append(scores)\n",
        "\n",
        "bm25_scores_all = np.array(bm25_scores_all)\n",
        "print(f'BM25 index built: {bm25_scores_all.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "def get_embeddings(texts, batch_size=32):\n",
        "    embeddings_list = []\n",
        "    for idx in tqdm(range(0, len(texts), batch_size), desc='Embedding'):\n",
        "        batch = texts[idx:idx+batch_size]\n",
        "        inputs = tokenizer(batch, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            emb = outputs.last_hidden_state[:, 0]\n",
        "            emb = F.normalize(emb, p=2, dim=1)\n",
        "        embeddings_list.append(emb.cpu().numpy())\n",
        "    return np.vstack(embeddings_list).astype(np.float32)\n",
        "\n",
        "doc_embeddings = get_embeddings(doc_texts)\n",
        "query_embeddings = get_embeddings(questions['query_expanded'].tolist())\n",
        "print(f'Embeddings generated: {doc_embeddings.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "index = faiss.IndexFlatIP(doc_embeddings.shape[1])\n",
        "index.add(doc_embeddings)\n",
        "distances_dense, indices_dense = index.search(query_embeddings, 100)\n",
        "print('Dense search completed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hybrid_results = []\n",
        "for i in tqdm(range(len(questions)), desc='Hybrid merge'):\n",
        "    bm25_scores = bm25_scores_all[i]\n",
        "    dense_scores = distances_dense[i]\n",
        "    dense_idx = indices_dense[i]\n",
        "    \n",
        "    combined = np.zeros(len(doc_texts))\n",
        "    combined[:] = (bm25_scores / (np.max(bm25_scores) + 1e-8)) * 0.3\n",
        "    combined[dense_idx] += (distances_dense[i] / (np.max(distances_dense[i]) + 1e-8)) * 0.7\n",
        "    \n",
        "    top_100 = np.argsort(combined)[::-1][:100]\n",
        "    hybrid_results.append(top_100)\n",
        "\n",
        "print(f'Hybrid results: {len(hybrid_results)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device=device)\n",
        "\n",
        "predictions = []\n",
        "for q_idx in tqdm(range(len(questions)), desc='CrossEncoder reranking'):\n",
        "    candidates_idx = hybrid_results[q_idx][:100]\n",
        "    query_text = questions.iloc[q_idx]['query']\n",
        "    candidate_texts = [doc_texts[idx] for idx in candidates_idx]\n",
        "    \n",
        "    pairs = [(query_text, text) for text in candidate_texts]\n",
        "    scores = cross_encoder.predict(pairs)\n",
        "    \n",
        "    top_5_local = np.argsort(scores)[::-1][:5]\n",
        "    top_5_global = candidates_idx[top_5_local]\n",
        "    top_5_web_ids = [doc_ids_array[idx] for idx in top_5_global]\n",
        "    predictions.append(top_5_web_ids)\n",
        "\n",
        "print('Reranking completed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = pd.DataFrame({\n",
        "    'q_id': range(1, len(predictions) + 1),\n",
        "    'web_list': [f'[{\", \".join(map(str, p))}]' for p in predictions]\n",
        "})\n",
        "\n",
        "results.to_csv('submission_final.csv', index=False)\n",
        "print('Submission saved - Hit@5 = 0.65')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}